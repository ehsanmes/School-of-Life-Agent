import os
import feedparser
import telegram
import asyncio
import time
import requests # <--- Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ requests Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from openai import OpenAI

# --- 1. CONFIGURATION ---
AVALAI_API_KEY = os.environ.get("AVALAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")
AVALAI_BASE_URL = "https://api.avalai.ir/v1"
MODEL_TO_USE = "gpt-4o-mini" 

# --- !!!!!!!!!!!!!!!!!!!!!!!!!!!!! ---
# --- Ø¢Ø¯Ø±Ø³ Ù…Ø³ØªÙ‚ÛŒÙ… ØµÙØ­Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª ---
ARTICLES_PAGE_URL = "https://www.theschooloflife.com/articles/"
# --- !!!!!!!!!!!!!!!!!!!!!!!!!!!!! ---

MEMORY_FILE = "_last_processed_link.txt"

# --- 2. INITIALIZE THE AI CLIENT ---
client = None
if AVALAI_API_KEY:
    try:
        client = OpenAI(api_key=AVALAI_API_KEY, base_url=AVALAI_BASE_URL)
        print("âœ… AvalAI client configured successfully.")
    except Exception as e:
        print(f"âŒ Critical error during client initialization: {e}")
else:
    print("âŒ AVALAI_API_KEY secret is not set.")

# --- 3. FUNCTIONS ---

def get_last_processed_link():
    """Ù„ÛŒÙ†Ú© Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ø­Ø§ÙØ¸Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯"""
    try:
        with open(MEMORY_FILE, 'r') as f:
            return f.read().strip()
    except FileNotFoundError:
        print("Memory file not found. Will create one.")
        return None

def set_last_processed_link(link):
    """Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø­Ø§ÙØ¸Ù‡ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ø¯"""
    try:
        with open(MEMORY_FILE, 'w') as f:
            f.write(link)
        print(f"Updated memory file with new link: {link}")
    except Exception as e:
        print(f"Error writing to memory file: {e}")

def get_newest_article_from_webpage(url):
    """Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ØµÙØ­Ù‡ ÙˆØ¨ Ø±Ø§ Ø§Ø³Ú©Ø±Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯"""
    print(f"Scraping webpage: {url}...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status() # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø·Ø§
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§ÛŒØª The School of LifeØŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø¯Ø± ØªÚ¯ <article> Ù‡Ø³ØªÙ†Ø¯
        latest_article_element = soup.find('article')
        
        if not latest_article_element:
            print("No <article> tag found on the page.")
            return None

        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ùˆ Ø¹Ù†ÙˆØ§Ù†
        link_tag = latest_article_element.find('a', href=True)
        title_tag = latest_article_element.find('h3') # ÛŒØ§ ØªÚ¯ Ø¹Ù†ÙˆØ§Ù† Ù…Ù†Ø§Ø³Ø¨ Ø¯ÛŒÚ¯Ø±
        
        if not link_tag or not title_tag:
            print("Could not find link or title tag within the article element.")
            return None

        article_link = link_tag['href']
        article_title = title_tag.get_text().strip()
        
        # Ø­Ø§Ù„Ø§ Ø¨Ù‡ ØµÙØ­Ù‡ Ø®ÙˆØ¯ Ù…Ù‚Ø§Ù„Ù‡ Ù…ÛŒâ€ŒØ±ÙˆÛŒÙ… ØªØ§ Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø±Ø§ Ø¨Ú¯ÛŒØ±ÛŒÙ…
        print(f"Found latest article: {article_title}. Fetching content...")
        article_response = requests.get(article_link, headers=headers, timeout=15)
        article_response.raise_for_status()
        
        article_soup = BeautifulSoup(article_response.content, 'html.parser')
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ø®Ø´ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡
        content_element = article_soup.find('div', class_='entry-content') # Ú©Ù„Ø§Ø³ Ù…Ø¹Ù…ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª
        if not content_element:
            content_element = article_soup.find('article') # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ ØªÚ¯ Ù…Ù‚Ø§Ù„Ù‡
            
        if not content_element:
            print("Could not find main content element on article page.")
            return None

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† ØªÙ…ÛŒØ²
        article_text = content_element.get_text(separator='\n', strip=True)
        
        article = {
            "title": article_title,
            "link": article_link,
            "content": article_text
        }
        return article
        
    except Exception as e: 
        print(f"Could not scrape webpage. Error: {e}")
        return None

def summarize_and_format(article):
    """Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØªØŒ Ø®Ù„Ø§ØµÙ‡ Ú©Ø±Ø¯Ù‡ØŒ Ù‡Ø´ØªÚ¯ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    if client is None: return "AI client is not available.", None
    print(f"Analyzing article: {article['title']}")
    
    system_message_summary = "Ø´Ù…Ø§ ÛŒÚ© Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ùˆ Ù…ØªÙÚ©Ø± Ø¹Ù…ÛŒÙ‚ Ù…Ø³Ù„Ø· Ø¨Ù‡ ÙÙ„Ø³ÙÙ‡ Ùˆ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø§Ø² Ø³Ø§ÛŒØª The School of Life Ùˆ Ù†ÙˆØ´ØªÙ† ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„ÛŒ Ø¹Ù…ÛŒÙ‚ Ùˆ Ù…ÙÙ‡ÙˆÙ…ÛŒ (Ø­Ø¯ÙˆØ¯ Û±ÛµÛ° ØªØ§ Û²Û°Û° Ú©Ù„Ù…Ù‡) Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª. Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§ÛŒØ¯ Ø±ÙˆØ§Ù†ØŒ Ø¬Ø°Ø§Ø¨ Ùˆ ÙÙ„Ø³ÙÛŒ Ø¨Ø§Ø´Ø¯ Ùˆ Ù…ÙØ§Ù‡ÛŒÙ… Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ù…Ù†ØªÙ‚Ù„ Ú©Ù†Ø¯. Ø§Ø² Ù…Ù‚Ø¯Ù…Ù‡â€ŒÚ†ÛŒÙ†ÛŒ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ø³Ø±Ø§Øº ØªØ­Ù„ÛŒÙ„ Ø¨Ø±ÙˆÛŒØ¯."
    user_message_summary = f"Please summarize this article in a detailed, fluid, and engaging Persian summary:\n\nTitle: {article['title']}\n\nContent:\n{article['content']}"
    
    persian_summary = ""
    try:
        completion_summary = client.chat.completions.create(model=MODEL_TO_USE, messages=[{"role": "system", "content": system_message_summary}, {"role": "user", "content": user_message_summary}], max_tokens=2048, temperature=0.7)
        persian_summary = completion_summary.choices[0].message.content.strip()
        print("âœ… Summary generated successfully.")
    except Exception as e:
        print(f"Could not analyze article. Error: {e}")
        return None, None

    print("Waiting for 5 seconds before generating hashtags...")
    time.sleep(5)
    
    system_message_hashtags = "You are a metadata specialist. Read the following text and generate exactly 5 relevant, single-word hashtags in Persian. Do not include the '#' symbol. Separate them with commas."
    user_message_hashtags = f"Text:\n{persian_summary}"
    
    hashtags_string = ""
    try:
        completion_tags = client.chat.completions.create(model=MODEL_TO_USE, messages=[{"role": "system", "content": system_message_hashtags}, {"role": "user", "content": user_message_hashtags}], max_tokens=100, temperature=0.2)
        tags = completion_tags.choices[0].message.content.strip()
        hashtags_string = " ".join([f"#{tag.strip().replace(' ', '_')}" for tag in tags.split(',')]) 
        print(f"âœ… Hashtags generated: {hashtags_string}")
    except Exception as e:
        print(f"Could not generate hashtags. Error: {e}")
        hashtags_string = "#Ø®Ù„Ø§ØµÙ‡" 

    final_post = (
        f"<b>{article['title']}</b>\n\n"
        f"{persian_summary}\n\n"
        f"{hashtags_string}\n\n"
        f"<a href='{article['link']}'>Ù…Ù†Ø¨Ø¹</a>\n"
        f"@momento_lab ğŸ’¡"
    )
    return final_post, article['link'] 
        
async def send_to_telegram(report, token, chat_id):
    if not token or not chat_id: print("Telegram secrets not found."); return
    print("Sending post to Telegram...")
    try:
        bot = telegram.Bot(token=token)
        await bot.send_message(chat_id=chat_id, text=report, parse_mode='HTML', disable_web_page_preview=True)
        print("Post successfully sent.")
    except Exception as e: print(f"Failed to send post. Error: {e}")

# --- 4. EXECUTION (with new scraping logic) ---
def main():
    if client is None:
        print("Agent will not run. Check API Key.")
        return

    last_link = get_last_processed_link()
    new_article = get_newest_article_from_webpage(ARTICLES_PAGE_URL) # <--- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯
    
    if new_article is None:
        print("No articles found on webpage. Stopping.")
        print("\n--- AGENT RUN FINISHED ---")
        return

    if new_article['link'] == last_link:
        print("Article is the same as last run. No new article found. Stopping.")
        print("\n--- AGENT RUN FINISHED ---")
        return
        
    print(f"New article found! Processing: {new_article['title']}")
    report, new_link = summarize_and_format(new_article)
    
    if report:
        asyncio.run(send_to_telegram(report, TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID))
        set_last_processed_link(new_link) # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
    else:
        print("Failed to generate report.")
        
    print("\n--- AGENT RUN FINISHED ---")

if __name__ == "__main__":
    main()
