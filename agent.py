import os
import feedparser
import telegram
import asyncio
import time
import random
from bs4 import BeautifulSoup
from openai import OpenAI
from datetime import datetime, timedelta

# --- 1. CONFIGURATION ---
AVALAI_API_KEY = os.environ.get("AVALAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")
AVALAI_BASE_URL = "https://api.avalai.ir/v1"
MODEL_TO_USE = "gpt-4o-mini" 

# --- !!!!!!!!!!!!!!!!!!!!!!!!!!!!! ---
# --- Ù…Ø®Ø²Ù† Ú©Ø§Ù…Ù„ ÙÛŒØ¯Ù‡Ø§ ---
LIVE_FEEDS = {
    "The Marginalian": "https://www.themarginalian.org/feed/",
    "Scientific American Mind & Brain": "http://rss.sciam.com/sciam/mind-and-brain",
}
LOCAL_XML_FILE = "content.xml" # Ø¨Ø§Ù†Ú© Ù…Ù‚Ø§Ù„Ø§Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†
MEMORY_FILE = "_posted_articles.txt" # ÙØ§ÛŒÙ„ Ø­Ø§ÙØ¸Ù‡
# --- !!!!!!!!!!!!!!!!!!!!!!!!!!!!! ---

# --- 2. INITIALIZE THE AI CLIENT ---
client = None
if AVALAI_API_KEY:
    try:
        client = OpenAI(api_key=AVALAI_API_KEY, base_url=AVALAI_BASE_URL)
        print("âœ… AvalAI client configured successfully.")
    except Exception as e:
        print(f"âŒ Critical error during client initialization: {e}")
else:
    print("âŒ AVALAI_API_KEY secret is not set.")

# --- 3. FUNCTIONS ---

def get_posted_links():
    """Ù„ÛŒØ³Øª Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø³Øª Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯"""
    try:
        with open(MEMORY_FILE, 'r') as f:
            return set(line.strip() for line in f.readlines() if line.strip())
    except FileNotFoundError:
        print("Memory file not found. Will create one.")
        return set()

def add_link_to_memory(link):
    """Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ø­Ø§ÙØ¸Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    try:
        with open(MEMORY_FILE, 'a') as f:
            f.write(link + '\n')
        print(f"Updated memory file with new link: {link}")
    except Exception as e:
        print(f"Error writing to memory file: {e}")

def get_all_unposted_articles(live_feeds, local_xml, posted_links):
    """ØªÙ…Ø§Ù… Ù…Ù‚Ø§Ù„Ø§Øª Ù¾Ø³Øª Ù†Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø² Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹ (Ø²Ù†Ø¯Ù‡ Ùˆ Ù…Ø­Ù„ÛŒ) Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    print("Fetching articles from all sources...")
    all_unposted_articles = []
    
    # --- Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø¨Ø±Ø±Ø³ÛŒ ÙÛŒØ¯Ù‡Ø§ÛŒ Ø²Ù†Ø¯Ù‡ ---
    for journal, url in live_feeds.items():
        print(f"Checking live feed: {journal}")
        try:
            feed = feedparser.parse(url)
            if not feed.entries: continue
                
            for entry in feed.entries:
                link = entry.link
                if link not in posted_links:
                    content_html = entry.get('content', [{}])[0].get('value', '')
                    if not content_html: content_html = getattr(entry, 'description', '')
                    summary_text = BeautifulSoup(content_html, 'html.parser').get_text()
                    
                    article = {"title": entry.title.strip(), "link": link, "content": summary_text, "source": journal}
                    all_unposted_articles.append(article)
                    
        except Exception as e: 
            print(f"Could not parse feed for {journal}. Error: {e}")

    # --- Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Ù†Ú© Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø­Ù„ÛŒ ---
    print(f"Checking local file: {local_xml}...")
    try:
        with open(local_xml, 'r', encoding='utf-8') as f:
            feed_content = f.read()
            
        feed = feedparser.parse(feed_content)
        if feed.entries:
            for entry in feed.entries:
                if "rss.app" in entry.link or "theschooloflife.com/articles/" == entry.link: continue
                
                link = entry.link
                if link not in posted_links:
                    content_html = entry.get('content', [{}])[0].get('value', '')
                    if not content_html: content_html = getattr(entry, 'description', '')
                    summary_text = BeautifulSoup(content_html, 'html.parser').get_text()
                    
                    article = {"title": entry.title.replace(" - The School of Life", ""), "link": link, "content": summary_text, "source": "The School of Life"}
                    all_unposted_articles.append(article)
        
    except FileNotFoundError:
        print(f"ERROR: '{local_xml}' not found. Skipping local DB.")
    except Exception as e: 
        print(f"Could not parse local XML file. Error: {e}")

    
    if not all_unposted_articles:
        print("No new (unposted) articles found in any source.")
        return None
        
    # --- Ø§Ù†ØªØ®Ø§Ø¨ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Ù…Ø®Ø²Ù† Ø¨Ø²Ø±Ú¯ ---
    chosen_article = random.choice(all_unposted_articles)
    return chosen_article

def summarize_and_format(article):
    """Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØªØŒ Ø®Ù„Ø§ØµÙ‡ Ú©Ø±Ø¯Ù‡ØŒ Ù‡Ø´ØªÚ¯ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    if client is None: return "AI client is not available.", None
    print(f"Analyzing article: {article['title']}")
    
    # --- STEP 1: Generate Summary ---
    system_message_summary = "Ø´Ù…Ø§ ÛŒÚ© Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ùˆ Ù…ØªÙÚ©Ø± Ø¹Ù…ÛŒÙ‚ Ù…Ø³Ù„Ø· Ø¨Ù‡ ÙÙ„Ø³ÙÙ‡ Ùˆ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ùˆ Ù†ÙˆØ´ØªÙ† ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„ÛŒ Ø¹Ù…ÛŒÙ‚ Ùˆ Ù…ÙÙ‡ÙˆÙ…ÛŒ (Ø­Ø¯ÙˆØ¯ Û²ÛµÛ° ØªØ§ Û³Û°Û° Ú©Ù„Ù…Ù‡) Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª. Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§ÛŒØ¯ Ø±ÙˆØ§Ù†ØŒ Ø¬Ø°Ø§Ø¨ Ùˆ ÙÙ„Ø³ÙÛŒ Ø¨Ø§Ø´Ø¯. Ù…ÙØ§Ù‡ÛŒÙ… Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ (Ù…Ø§Ù†Ù†Ø¯ ğŸ’¡, ğŸ¯, ğŸ§ ) Ø¨Ù‡ Ø¬Ø§ÛŒ Ø¨ÙˆÙ„Øª Ù¾ÙˆÛŒÙ†ØªØŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¨Ø§Ù„Ø§ Ø¨Ø±ÙˆØ¯. Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø®Ù„Ø§ØµÙ‡ Ø±Ø§ Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯ Ùˆ Ù‡ÛŒÚ† Ù…Ù‚Ø¯Ù…Ù‡ ÛŒØ§ ØªÙˆØ¶ÛŒØ­ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú©Ø§Ø±ÛŒ Ú©Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯ Ù†Ù†ÙˆÛŒØ³ÛŒØ¯."
    user_message_summary = f"Please summarize this article in a detailed, 250-300 word, fluid, and engaging Persian summary, using emojis for key concepts:\n\nTitle: {article['title']}\n\nContent:\n{article['content']}"
    
    persian_summary = ""
    try:
        completion_summary = client.chat.completions.create(model=MODEL_TO_USE, messages=[{"role": "system", "content": system_message_summary}, {"role": "user", "content": user_message_summary}], max_tokens=3000, temperature=0.7)
        persian_summary = completion_summary.choices[0].message.content.strip()
        print("âœ… Summary generated successfully.")
    except Exception as e:
        print(f"Could not analyze article. Error: {e}")
        return None, None

    # --- STEP 2: Translate Title ---
    print("Waiting for 5 seconds...")
    time.sleep(5)
    system_message_title = "Translate the following English title to Persian. Only return the translated text, nothing else."
    user_message_title = article['title']
    persian_title = article['title'] 
    try:
        completion_title = client.chat.completions.create(model=MODEL_TO_USE, messages=[{"role": "system", "content": system_message_title}, {"role": "user", "content": user_message_title}], max_tokens=100, temperature=0.1)
        persian_title = completion_title.choices[0].message.content.strip()
        print(f"âœ… Title translated successfully: {persian_title}")
    except Exception as e:
        print(f"Could not translate title. Error: {e}")

    # --- STEP 3: Generate Hashtags ---
    print("Waiting for 5 seconds...")
    time.sleep(5)
    system_message_hashtags = "You are a metadata specialist. Read the following text and generate exactly 5 relevant, single-word hashtags in Persian. Do not include the '#' symbol. Separate them with commas."
    user_message_hashtags = f"Text:\n{persian_summary}"
    hashtags_string = ""
    try:
        completion_tags = client.chat.completions.create(model=MODEL_TO_USE, messages=[{"role": "system", "content": system_message_hashtags}, {"role": "user", "content": user_message_hashtags}], max_tokens=100, temperature=0.2)
        tags = completion_tags.choices[0].message.content.strip()
        hashtags_string = " ".join([f"#{tag.strip().replace(' ', '_')}" for tag in tags.split(',')]) 
        print(f"âœ… Hashtags generated: {hashtags_string}")
    except Exception as e:
        print(f"Could not generate hashtags. Error: {e}")
        hashtags_string = f"#{article['source'].lower().replace(' ', '')}" 

    # --- STEP 4: Assemble Final Post ---
    final_post = (
        f"<b>{persian_title}</b>\n\n" 
        f"{persian_summary}\n\n"
        f"{hashtags_string}\n\n"
        f"<i>Ù…Ù†Ø¨Ø¹: {article['source']}</i>\n"
        f"<a href='{article['link']}'>Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø·Ù„Ø¨</a>\n"
        f"@momento_lab ğŸ’¡"
    )
    return final_post, article['link']
        
async def send_to_telegram(report, token, chat_id):
    if not token or not chat_id: print("Telegram secrets not found."); return
    print("Sending post to Telegram...")
    try:
        bot = telegram.Bot(token=token)
        await bot.send_message(
            chat_id=chat_id, 
            text=report, 
            parse_mode='HTML',
            disable_web_page_preview=False
        )
        print("Post successfully sent.")
    except Exception as e: print(f"Failed to send post. Error: {e}")

# --- 5. EXECUTION (with combined logic) ---
def main():
    if client is None:
        print("Agent will not run. Check API Key.")
        return

    posted_links = get_posted_links()
    
    # Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„: ÙÛŒØ¯Ù‡Ø§ÛŒ Ø²Ù†Ø¯Ù‡
    new_article = get_all_unposted_articles(LIVE_FEEDS, LOCAL_XML_FILE, posted_links)
    
    if new_article is None:
        print("No new (unposted) articles found in any source. Stopping.")
        print("\n--- AGENT RUN FINISHED ---")
        return
        
    print(f"New article selected! Processing: {new_article['title']}")
    report, new_link = summarize_and_format(new_article)
    
    if report:
        asyncio.run(send_to_telegram(report, TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID))
        add_link_to_memory(new_link) 
    else:
        print("Failed to generate report.")
        
    print("\n--- AGENT RUN FINISHED ---")

if __name__ == "__main__":
    main()
